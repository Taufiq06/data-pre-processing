{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import numpy as np\n",
    "from carto.datasets import DatasetManager\n",
    "from carto.auth import APIKeyAuthClient\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of table on Carto where you want to upload data\n",
    "# this should be a table name that is not currently in use\n",
    "dataset_name = 'soc_025a_gender_inequality_index' #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory that you are working in with the path variable\n",
    "dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new sub-directory within your specified dir called 'data'\n",
    "# within this directory, create files to store raw and processed data\n",
    "data_dir = 'data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data from the source\n",
    "url = 'http://hdr.undp.org/sites/default/files/hdro_statistical_data_table_5.xlsx'\n",
    "raw_data_file = data_dir+os.path.basename(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process data\n",
    "#Create URL to Excel file (alternatively this can be a filepath)\n",
    "#read in data to pandas dataframe\n",
    "url = 'http://hdr.undp.org/sites/default/files/hdro_statistical_data_table_5.xlsx'\n",
    "r = requests.get(url)\n",
    "df = pd.read_excel(io.BytesIO(r.content), encoding='utf8')\n",
    "df.to_excel('data/hdro_statistical_data_table_5.xlsx',header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns without data\n",
    "df = df.drop(df.columns.difference(['Unnamed: 0','Table 5. Gender Inequality Index', 'Unnamed: 2','Unnamed: 4',\n",
    "                                    'Unnamed: 6', 'Unnamed: 8','Unnamed: 10','Unnamed: 12', 'Unnamed: 14', 'Unnamed: 16', 'Unnamed: 18']), 1)\n",
    "df = df.iloc[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.columns = ['HDI rank','Country','2018_GIIvalue', '2018_GIIrank', '2015 Maternal Mortality (per 1000 births)', \n",
    "              '2015-2020 Adolescent birth rate (births per 1,000 women ages 15â€“19)', '2018 Share of seats in parliament',\n",
    "            '2010-2018 fem with secondary ed', '2010-2018 male with secondary ed', '2018 fem labor', '2018 male labor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values (NaN)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values (NaN)\n",
    "null_data = df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all '..' with None\n",
    "df = df.replace({'..':None})\n",
    "#replace all NaN with None\n",
    "final_df=df.where((pd.notnull(df)), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save processed dataset to csv\n",
    "csv_loc = data_dir+dataset_name+'_edit.csv'\n",
    "final_df.to_csv(csv_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload processed data to Carto\n",
    "print('Uploading processed data to Carto.')\n",
    "#set up carto authentication using local variables for username (CARTO_WRI_RW_USER) and API key (CARTO_WRI_RW_KEY)\n",
    "auth_client = APIKeyAuthClient(api_key=os.getenv('CARTO_WRI_RW_KEY'), base_url=\"https://{user}.carto.com/\".format(user=os.getenv('CARTO_WRI_RW_USER')))\n",
    "print (os.getenv('CARTO_WRI_RW_KEY'))\n",
    "#set up dataset manager with authentication\n",
    "dataset_manager = DatasetManager(auth_client)\n",
    "#upload dataset to carto\n",
    "dataset = dataset_manager.create(csv_loc)\n",
    "print('Carto table created: {}'.format(os.path.basename(csv_loc).split('.')[0]))\n",
    "#set dataset privacy to 'Public with link'\n",
    "dataset.privacy = 'LINK'\n",
    "dataset.save()\n",
    "print('Privacy set to public with link.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload original data and processed data to Amazon S3 storage\n",
    "def upload_to_aws(local_file, bucket, s3_file):\n",
    "    s3 = boto3.client('s3', aws_access_key_id=os.getenv('aws_access_key_id'), aws_secret_access_key=os.getenv('aws_secret_access_key'))\n",
    "    try:\n",
    "        s3.upload_file(local_file, bucket, s3_file)\n",
    "        print(\"Upload Successful\")\n",
    "        print(\"http://{}.s3.amazonaws.com/{}\".format(bucket, s3_file))\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found\")\n",
    "        return False\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return False\n",
    "\n",
    "print('Uploading original data to S3.')\n",
    "# Copy the raw data into a zipped file to upload to S3\n",
    "raw_data_dir = data_dir+dataset_name+'.zip'\n",
    "\n",
    "with ZipFile(raw_data_dir,'w') as zip:\n",
    "    zip.write(raw_data_file, os.path.basename(raw_data_file))\n",
    "    \n",
    "# Upload raw data file to S3\n",
    "uploaded = upload_to_aws(raw_data_dir, 'wri-public-data', 'resourcewatch/'+os.path.basename(raw_data_dir))\n",
    "\n",
    "print('Uploading processed data to S3.')\n",
    "# Copy the processed data into a zipped file to upload to S3\n",
    "processed_data_dir = data_dir+dataset_name+'_edit'+'.zip'\n",
    "with ZipFile(processed_data_dir,'w') as zip:\n",
    "    zip.write(csv_loc, os.path.basename(csv_loc))\n",
    "\n",
    "# Upload processed data file to S3\n",
    "uploaded = upload_to_aws(processed_data_dir, 'wri-public-data', 'resourcewatch/'+os.path.basename(processed_data_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
